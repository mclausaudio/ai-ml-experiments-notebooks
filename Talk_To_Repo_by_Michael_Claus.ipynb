{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "agVhFRlhBfNa"
      ],
      "authorship_tag": "ABX9TyNTT5+YJ4jh6w+AO2lXFMqk",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mclausaudio/ai-ml-experiments-notebooks/blob/main/Talk_To_Repo_by_Michael_Claus.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This notebook will walk you through how you can use LangChain and Pinecone DB to \"talk\" to a GitHub repo.  \n",
        "\n",
        "Read more about this project on my blog:\n",
        "\n",
        "https://medium.com/@michaelclaus/my-attempt-to-talk-to-a-github-repo-using-python-langchain-pinecone-db-and-openai-73b6df90d0e9"
      ],
      "metadata": {
        "id": "J7jb-rpVMpaf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Install dependencies\n",
        "pip install time"
      ],
      "metadata": {
        "id": "0oLBJE76rJyg"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AJ7_I1dmmB3X"
      },
      "outputs": [],
      "source": [
        "!pip install langchain\n",
        "!pip install pinecone-client\n",
        "!pip install openai\n",
        "!pip install tiktoken"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Add your OpenAI keys\n",
        "We load in our OpenAI key below"
      ],
      "metadata": {
        "id": "hBsn61lwr-0S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "# Add your OpenAI key here\n",
        "os.environ[\"OPENAI_API_KEY\"] = \"\""
      ],
      "metadata": {
        "id": "HK8w3B1_3FnW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Clone to repo you want to talk to\n",
        "Clone your repo into this project directory.  Update the URL to the URL you would like to talk to.  It will download the repo in a directory called `codebase`."
      ],
      "metadata": {
        "id": "AfFqfatlrQiL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/mclausaudio/notion-personal-assistant-ai codebase"
      ],
      "metadata": {
        "id": "Pt6U2LQam5dY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Convert repo to text, prepare it to be vectorized\n",
        "Below loops over the repo you just cloned and converts it all to `.txt` files and placed into a new directory called `converted_codebase`.  `converted_codebase` will essentially be a replica of `codebase` except it will be all `.txt` files.  It preserves the original extension and basically appends `.txt` to it.  So for example, `index.js` will become `index.js.txt`."
      ],
      "metadata": {
        "id": "RJGmXngPrblh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def convert_files_to_txt(src_dir, dst_dir):\n",
        "    # If the destination directory does not exist, create it.\n",
        "    if not os.path.exists(dst_dir):\n",
        "        os.makedirs(dst_dir)\n",
        "\n",
        "    for root, dirs, files in os.walk(src_dir):\n",
        "        for file in files:\n",
        "            file_path = os.path.join(root, file)\n",
        "            rel_path = os.path.relpath(file_path, src_dir)  # get the relative path to preserve directory structure\n",
        "\n",
        "            # Create the same directory structure in the new directory\n",
        "            new_root = os.path.join(dst_dir, os.path.dirname(rel_path))\n",
        "            os.makedirs(new_root, exist_ok=True)\n",
        "\n",
        "            try:\n",
        "                with open(file_path, 'r', encoding='utf-8') as f:\n",
        "                    data = f.read()\n",
        "            except UnicodeDecodeError:\n",
        "                try:\n",
        "                    with open(file_path, 'r', encoding='latin-1') as f:\n",
        "                        data = f.read()\n",
        "                except UnicodeDecodeError:\n",
        "                    print(f\"Failed to decode the file: {file_path}\")\n",
        "                    continue\n",
        "\n",
        "            # Create a new file path with .txt extension\n",
        "            new_file_path = os.path.join(new_root, file + '.txt')\n",
        "            with open(new_file_path, 'w', encoding='utf-8') as f:\n",
        "                f.write(data)\n",
        "\n",
        "# Call the function with the source and destination directory paths\n",
        "convert_files_to_txt('/content/codebase', '/content/converted_codebase')\n"
      ],
      "metadata": {
        "id": "ijphoZ9XrZFn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Pinecone DB time!\n",
        "Next we loop over `converted_codebase` and load, split the contents of each file into chunks, create a vector representation of the pieces of text (embeddings) and write into Pinecone DB.\n",
        "\n",
        "Check your Pinecone DB index.  If it already has vectors in it (as in you've already ran this cell) you don't need to run it again.  If you want to try a new `chunk_size` or `chunk_overlap` value, you can delete and recreate your index.  Or you could keep adding more vectors into the index, although I think that might make it perform worse (could be wrong).\n",
        "\n",
        "### Important Notes\n",
        "- When you configure your PineconeDB index, be sure you set `Dimensions` to `1536`.  As you can see in the [OpenAI docs](https://platform.openai.com/docs/guides/embeddings/second-generation-models)\n",
        "- Experiment with the `chunk_size` and `chunk_overlap` to yield more granular results at the cost of longer write to DB time.\n",
        "- Useful docs: https://python.langchain.com/docs/modules/data_connection/"
      ],
      "metadata": {
        "id": "xCgaz506sDpJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.document_loaders import DirectoryLoader, TextLoader\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.embeddings import OpenAIEmbeddings\n",
        "from langchain.vectorstores import Pinecone\n",
        "import pinecone\n",
        "\n",
        "PINECONE_API_KEY = \"\"\n",
        "PINECONE_ENVIRONMENT = \"\"\n",
        "PINECONE_INDEX_NAME = \"\"\n",
        "\n",
        "pinecone.init(api_key=PINECONE_API_KEY, environment=PINECONE_ENVIRONMENT)\n",
        "\n",
        "def ingest_files(src_dir):\n",
        "    loader = DirectoryLoader(src_dir, show_progress=True, loader_cls=TextLoader)\n",
        "    repo_files = loader.load()\n",
        "    print(f\"Number of files loaded: {len(repo_files)}\")\n",
        "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=100, chunk_overlap=50)\n",
        "    documents = text_splitter.split_documents(documents=repo_files)\n",
        "    print(f\"Number of documents : {len(documents)}\")\n",
        "    for doc in documents:\n",
        "      old_path_with_txt_extension = doc.metadata[\"source\"]\n",
        "      new_path_without_txt_extension = old_path_with_txt_extension.replace(\".txt\", \"\")\n",
        "      doc.metadata.update({\"source\": new_path_without_txt_extension})\n",
        "\n",
        "    print(f\"Going in insert {len(documents)} to pinecone\")\n",
        "    embeddings = OpenAIEmbeddings()\n",
        "    Pinecone.from_documents(documents, embeddings, index_name=PINECONE_INDEX_NAME)\n",
        "    print(f\"Done inserting to pinecone\")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "ingest_files('/content/converted_codebase')\n",
        "\n"
      ],
      "metadata": {
        "id": "a_ticVlBmFgU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Embed Pinecone DB Index with LangChain's RetrievalQA Chain\n",
        "We set up the LLM so we can talk to it.  I tried using the `RetrievalQA` chain, as well as teh `RetrievalQAWithSourcesChain`.  I liked the latter, because it cites it's sources."
      ],
      "metadata": {
        "id": "Zi4hM6twvuUO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# from langchain.chains import RetrievalQA\n",
        "from langchain.chains import RetrievalQAWithSourcesChain\n",
        "from langchain.chat_models import ChatOpenAI\n",
        "\n",
        "embeddings = OpenAIEmbeddings(openai_api_key=os.environ[\"OPENAI_API_KEY\"])\n",
        "docsearch = Pinecone.from_existing_index(PINECONE_INDEX_NAME, embeddings)\n",
        "\n",
        "llm = ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0)\n",
        "embeddings = OpenAIEmbeddings(openai_api_key=os.environ[\"OPENAI_API_KEY\"])\n",
        "# chain = RetrievalQA.from_chain_type(llm=llm, chain_type=\"stuff\", retriever=docsearch.as_retriever(), input_key=\"question\")\n",
        "chain = RetrievalQAWithSourcesChain.from_chain_type(llm=llm, chain_type=\"stuff\", retriever=docsearch.as_retriever())\n",
        "template = None"
      ],
      "metadata": {
        "id": "-muNuJ6lE8s-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Set up a prompt template (optional)\n",
        "We will set up a prompt template to wrap the users input.\n",
        "\n",
        "### Important note:\n",
        "I found that this actually made the LLM's responses WORSE!  I would recommend that you don't run the cell below"
      ],
      "metadata": {
        "id": "agVhFRlhBfNa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain import PromptTemplate\n",
        "\n",
        "\n",
        "template = \"\"\"\n",
        "You are a senior engineer who knows everything about the embedded repo. Your job is to act as a git repository assistant.\n",
        "You will answer the users questions in as much detail as possible using only information from the embedded GitHub repo.\n",
        "You are a programmer, so you are able to provide insight into how the repo works and could be updated, if the user asks those types of questions.\n",
        "You must provide as much detail as possible when answering the question and you must only reference files and information contained within the repo.\n",
        "\n",
        "Please answer the users question: {question}\n",
        "\"\"\"\n",
        "prompt = PromptTemplate.from_template(template)"
      ],
      "metadata": {
        "id": "yQ6sA677Be9d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Talk to the repo!\n",
        "Ask it some questions by updating `query`\n",
        "Note: If you switched the cell above to use `RetrievalQA` you will need to update the cell below accordingly, as the two chains use different `key`s in the query."
      ],
      "metadata": {
        "id": "qbDNGr98wAxq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Update the value of `query` with your question!\n",
        "query = \"Which frontend file handles users text input?  And how can I make the text areas background blue?\"\n",
        "\n",
        "if template is not None and prompt:\n",
        "  optimized_prompt = prompt.format(question=query)\n",
        "  result = chain({\"question\": optimized_prompt}, return_only_outputs=True)\n",
        "else:\n",
        "  result = chain({\"question\": query}, return_only_outputs=True)\n",
        "\n",
        "print(result)\n"
      ],
      "metadata": {
        "id": "GYJfB_mUwAA2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Helper function (you can ignore the cell below)\n",
        "Below is just a little helper code cell.  In case you want to delete some directorys, comment in / out as needed and run"
      ],
      "metadata": {
        "id": "oJSFsQWmwzVA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# !rm -r /content/codebase\n",
        "!rm -r /content/converted_codebase/\n"
      ],
      "metadata": {
        "id": "nYXZuU-GuigG"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}